{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../src')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from preprocessing import *\n",
    "from sklearn.pipeline import Pipeline as sklearn_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform titles\n",
      "Transform texts\n",
      "./Step3_PreProc.ipynb: ./Step1_DataAnalysis.ipynb: ./LabelCandidateExtract.ipynb: ./bigartm.WARNING: ./bigartm.ga.arina.log.WARNING.20190825-210857.6863: ./Step2_CreateDataBase.ipynb: ./Step3_PreProc.ipynb: ./Step1_DataAnalysis.ipynb: ./LabelCandidateExtract.ipynb: ./bigartm.WARNING: ./bigartm.ga.arina.log.WARNING.20190825-210857.6863: ./Step2_CreateDataBase.ipynb: ./Step3_PreProc.ipynb: ./Step1_DataAnalysis.ipynb: ./LabelCandidateExtract.ipynb: ./bigartm.WARNING: ./bigartm.ga.arina.log.WARNING.20190825-210857.6863: ./Step2_CreateDataBase.ipynb: ./Step3_PreProc.ipynb: ./Step1_DataAnalysis.ipynb: ./LabelCandidateExtract.ipynb: ./bigartm.WARNING: ./bigartm.ga.arina.log.WARNING.20190825-210857.6863: ./Step2_CreateDataBase.ipynb: ./Step3_PreProc.ipynb: ./Step1_DataAnalysis.ipynb: ./LabelCandidateExtract.ipynb: ./bigartm.WARNING: ./bigartm.ga.arina.log.WARNING.20190825-210857.6863: ./Step2_CreateDataBase.ipynb: ./Step3_PreProc.ipynb: ./Step1_DataAnalysis.ipynb: ./LabelCandidateExtract.ipynb: ./bigartm.WARNING: ./bigartm.ga.arina.log.WARNING.20190825-210857.6863: ./Step2_CreateDataBase.ipynb: ./Step3_PreProc.ipynb: ./Step1_DataAnalysis.ipynb: ./LabelCandidateExtract.ipynb: ./bigartm.WARNING: ./bigartm.ga.arina.log.WARNING.20190825-210857.6863: ./Step2_CreateDataBase.ipynb: "
     ]
    }
   ],
   "source": [
    "articles = pd.read_csv('../data/interim/articles_new.csv')\n",
    "\n",
    "chars_map = {'\\xad': ' ',\n",
    "             '…': '...',\n",
    "             '«': '', '»': '',\n",
    "             '\"': '', '\\'': '',\n",
    "             '’': '', '‘': '',\n",
    "             '”': '', '“': '', '„': '',\n",
    "             '`': '', '*': '', '_': '',\n",
    "             'http://':'', 'https://':''}\n",
    "\n",
    "pipeline = sklearn_pipeline([('replace_chars', ReplaceChars(chars_map)),\n",
    "                             ('sub_code', RegExprSub(r'\\{.*\\}', ' ')),\n",
    "                             ('sub_colon', RegExprSub(r'\\d*\\:\\d*', ' ')),\n",
    "                             ('sub_spaces', RegExprSub(r' +', ' ')),\n",
    "                             ('strip', Strip()),\n",
    "                             ('sent_tokenize', RusSentTokenizer()),\n",
    "                             ('spell', Yandex_Speller()),\n",
    "                             ('word_tokenize', Spacy_RusWordTokenizer()),\n",
    "                             ('space_detect', SpaceDetecter()),\n",
    "                             ('morph_predict', MorphPredictor()),\n",
    "                             ('conllu_encode', CoNLLUFormatEncoder()),\n",
    "                             ('syntax_parse', SyntaxParser('../models/parser_model.udpipe'))])\n",
    "\n",
    "print('Transform titles')\n",
    "articles['preproc_title'] = pipeline.fit_transform(articles.title)\n",
    "\n",
    "print('Transform texts')\n",
    "articles['preproc_text'] = pipeline.fit_transform(articles.text)\n",
    "\n",
    "articles.to_csv('../data/interim/articles_preproc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('../data/interim/articles_preproc.csv')\n",
    "\n",
    "upos_set={'ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN', 'VERB'}\n",
    "pipeline = sklearn_pipeline([('conllu_decode', CoNLLUFormatDecoder()),\n",
    "                             ('morph_filtration', MorphFilter(upos_set=upos_set)),\n",
    "                             ('vowpal_wabbit_encode', VowpalWabbitFormatEncoder())])\n",
    "\n",
    "articles['vw_title'] = pipeline.fit_transform(articles.preproc_title)\n",
    "articles['vw_text'] = pipeline.fit_transform(articles.preproc_text)\n",
    "\n",
    "with open('../data/interim/articles_vw.txt', 'w') as fl:\n",
    "    fl.write('\\n'.join([f'{article_id} |title {title} |text {text}' \n",
    "                        for article_id, title, text in \n",
    "                        articles[['id', 'vw_title', 'vw_text']].values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n"
     ]
    }
   ],
   "source": [
    "print('Finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
