{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import artm\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../src')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from preprocessing import *\n",
    "from sklearn.pipeline import Pipeline as sklearn_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/arina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/arina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/arina/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/arina/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0901 16:35:48.626969 139835170060096 deprecation_wrapper.py:119] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "2019-09-01 16:35:48.837 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 112: [loading vocabulary from /home/arina/.deeppavlov/models/ner_rus_bert/tag.dict]\n",
      "I0901 16:35:48.837283 139835170060096 simple_vocab.py:112] [loading vocabulary from /home/arina/.deeppavlov/models/ner_rus_bert/tag.dict]\n",
      "W0901 16:35:48.840496 139835170060096 deprecation_wrapper.py:119] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:38: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0901 16:35:48.841243 139835170060096 deprecation_wrapper.py:119] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:223: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0901 16:35:48.841571 139835170060096 deprecation_wrapper.py:119] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:223: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0901 16:35:48.849939 139835170060096 deprecation_wrapper.py:119] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:194: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0901 16:35:48.857017 139835170060096 deprecation_wrapper.py:119] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/deeppavlov/models/bert/bert_ner.py:126: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0901 16:35:48.977966 139835170060096 deprecation_wrapper.py:119] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/deeppavlov/models/bert/bert_ner.py:227: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0901 16:35:48.991203 139835170060096 deprecation_wrapper.py:119] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0901 16:35:49.344462 139835170060096 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0901 16:35:49.354333 139835170060096 deprecation.py:506] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0901 16:35:49.377797 139835170060096 deprecation.py:323] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0901 16:35:50.897431 139835170060096 deprecation.py:506] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0901 16:35:50.919071 139835170060096 deprecation.py:323] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/deeppavlov/models/bert/bert_ner.py:350: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0901 16:35:51.037862 139835170060096 deprecation.py:323] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0901 16:35:55.692027 139835170060096 deprecation.py:323] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0901 16:36:01.932809 139835170060096 deprecation.py:323] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/deeppavlov/models/bert/bert_ner.py:139: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "2019-09-01 16:36:01.934 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 52: [loading model from /home/arina/.deeppavlov/models/ner_rus_bert/model]\n",
      "I0901 16:36:01.934280 139835170060096 tf_model.py:52] [loading model from /home/arina/.deeppavlov/models/ner_rus_bert/model]\n",
      "W0901 16:36:01.935851 139835170060096 deprecation_wrapper.py:119] From /home/arina/anaconda3/envs/py36/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:55: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chars_map = {'\\xad': ' ',\n",
    "             '…': '...',\n",
    "             '«': '', '»': '',\n",
    "             '\"': '', '\\'': '',\n",
    "             '’': '', '‘': '',\n",
    "             '”': '', '“': '', '„': '',\n",
    "             '`': '', '*': '', '_': '', '©': '',\n",
    "             'http://':'', 'https://':'',\n",
    "             'тыс.': 'тыс. ', 'кв.': 'кв. ', 'куб.': 'куб. ',\n",
    "             'прим.': 'прим. ', 'Прим.': 'Прим.', 'зам.': 'зам. '}\n",
    "\n",
    "pipeline = sklearn_pipeline([('replace_chars', ReplaceChars(chars_map)),\n",
    "                             ('replace_part', ReplacePart(r'[а-яйё]+\\.[А-ЯЙЁ]+[а-яйё]+', lambda x: x.replace('.', '. '))),\n",
    "                             ('sub_code', RegExprSub(r'\\{.*\\}', ' ')),\n",
    "                             ('sub_colon', RegExprSub(r'\\d*\\:\\d*', ' ')),\n",
    "                             ('sub_round_brackets_without_words', RegExprSub(r'\\([^a-zA-Zа-яйёА-ЯЙЁ]+\\)', ' ')),\n",
    "                             ('sub_spaces', RegExprSub(r' +', ' ')),\n",
    "                             ('strip', Strip()),\n",
    "                             ('sent_tokenize', RusSentTokenizer()),\n",
    "                             ('sub_begin_hyphen', RegExprSub(r'^ *\\- *', '', sent=True)),\n",
    "                             ('ner_word_tokenize', NER_RusWordTokenizer()),\n",
    "                             ('morph_predict', MorphPredictor()),\n",
    "                             ('space_detect', SpaceDetecter()),\n",
    "                             ('ner_corrector', NER_Correcter()),\n",
    "                             ('conllu_encode', CoNLLUFormatEncoder())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform titles\n",
      "ReplaceChars\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 258097.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplacePart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 377931.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegExprSub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 977096.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegExprSub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 454129.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegExprSub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 974627.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegExprSub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 348856.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 1284719.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RusSentTokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 262846.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegExprSub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 534324.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER_RusWordTokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [06:49<00:00, 27.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MorphPredictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [02:30<00:00, 74.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaceDetecter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 186112.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER_Correcter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 121329.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoNLLUFormatEncoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 52883.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform texts\n",
      "ReplaceChars\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 79982.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplacePart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 27636.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegExprSub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 624749.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegExprSub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 45381.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegExprSub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 604667.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegExprSub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 29342.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 924506.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RusSentTokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:04<00:00, 2497.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegExprSub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 76532.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER_RusWordTokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [09:47<00:00, 22.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MorphPredictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [44:13<00:00,  6.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaceDetecter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 11736.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER_Correcter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 11847.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoNLLUFormatEncoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:04<00:00, 2694.75it/s]\n"
     ]
    }
   ],
   "source": [
    "articles = pd.read_csv('../data/interim/articles_new.csv')\n",
    "\n",
    "print('Transform titles')\n",
    "articles['preproc_title'] = pipeline.fit_transform(articles.title)\n",
    "\n",
    "print('Transform texts')\n",
    "articles['preproc_text'] = pipeline.fit_transform(articles.text)\n",
    "\n",
    "articles.to_csv('../data/interim/articles_preproc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoNLLUFormatDecoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 42145.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MorphFilter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 422620.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoNLLUFormatDecoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:06<00:00, 1663.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MorphFilter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11127/11127 [00:00<00:00, 35929.51it/s]\n"
     ]
    }
   ],
   "source": [
    "articles = pd.read_csv('../data/interim/articles_preproc.csv')\n",
    "\n",
    "pos_set = {'ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN', 'VERB'}\n",
    "pipeline = sklearn_pipeline([('conllu_decode', CoNLLUFormatDecoder()),\n",
    "                             ('morph_filtration', MorphFilter(pos_set=pos_set))])\n",
    "\n",
    "titles = pipeline.fit_transform(articles.preproc_title)\n",
    "texts = pipeline.fit_transform(articles.preproc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artm.BatchVectorizer(data_path=\"../data/interim/batches\", num_batches=12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "n_per = 0\n",
    "n_loc = 0\n",
    "n_org = 0\n",
    "\n",
    "articles_wv = []\n",
    "for article_id, title, text in zip(articles.id, titles, texts):\n",
    "    \n",
    "    title_tokens = []\n",
    "    text_tokens = []\n",
    "    \n",
    "    pers = []\n",
    "    locs = []\n",
    "    orgs = []\n",
    "    \n",
    "    for sent in title:\n",
    "        for token in sent.tokens:\n",
    "            if token.ne == 'O':\n",
    "                title_tokens.append(token.lemma)\n",
    "            elif token.ne[2:] == 'PER':\n",
    "                pers.append(token.lemma.replace(' ', '_'))\n",
    "            elif token.ne[2:] == 'LOC':\n",
    "                locs.append(token.lemma.replace(' ', '_'))\n",
    "            elif token.ne[2:] == 'ORG':\n",
    "                orgs.append(token.lemma.replace(' ', '_'))\n",
    "            else:\n",
    "                print(token.ne)\n",
    "    \n",
    "    for sent in text:\n",
    "        for token in sent.tokens:\n",
    "            if token.ne == 'O':\n",
    "                text_tokens.append(token.lemma)\n",
    "            elif token.ne[2:] == 'PER':\n",
    "                pers.append(token.lemma.replace(' ', '_'))\n",
    "            elif token.ne[2:] == 'LOC':\n",
    "                locs.append(token.lemma.replace(' ', '_'))\n",
    "            elif token.ne[2:] == 'ORG':\n",
    "                orgs.append(token.lemma.replace(' ', '_'))\n",
    "            else:\n",
    "                print(token.ne)\n",
    "                \n",
    "    n_per += len(pers)\n",
    "    n_loc += len(locs)\n",
    "    n_org += len(orgs)\n",
    "                \n",
    "    title_tokens = Counter(title_tokens)\n",
    "    text_tokens = Counter(text_tokens)\n",
    "    \n",
    "    pers = Counter(pers)\n",
    "    locs = Counter(locs)\n",
    "    orgs = Counter(orgs)\n",
    "    \n",
    "    encode = lambda x: ' '.join([f'{token}' + (f':{count}' if count > 1 else '') for token, count in x.items()])\n",
    "    \n",
    "    title_tokens = encode(title_tokens)\n",
    "    text_tokens = encode(text_tokens)\n",
    "    \n",
    "    pers = encode(pers)\n",
    "    locs = encode(locs)\n",
    "    orgs = encode(orgs)\n",
    "    \n",
    "    articles_wv.append(f'{article_id} |per {pers} |loc {locs} |org {orgs} |title {title_tokens} |text {text_tokens}')\n",
    "    \n",
    "with open('../data/interim/articles_vw.txt', 'w') as fl:\n",
    "    fl.write('\\n'.join(articles_wv))\n",
    "    \n",
    "artm.BatchVectorizer(data_path='../data/interim/articles_vw.txt', data_format='vowpal_wabbit', target_folder='../data/interim/batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50145 70360 53407\n",
      "0 |per медведев:2 дмитрий_медведев |loc россия дальневосточный_федеральный_округ |org  |title год льготный автокредит лизинг быть выделенный миллиард рубль |text год:5 льготный:4 автокредит:2 лизинг:4 быть:5 выделенный:3 миллиард:8 рубль:9 премьер заявить сегодня:2 ход заседание правительство:2 резерв выделять деньга поддержка автомобильный лёгкий промышленность миллион:2 более половина сумма район пойти программа:2 кредитование транспортный:2 средство:3 помочь сохранить кредитный лизинговый ставка нормальный уровень поддержать спрос автомобиль:3 делать последний рассчитывать рамка проданный менее лишний тысяча вид техника:3 сказать глава слово также:2 субсидия распределяться производитель газомоторный:2 проект распоряжение предусматриваться выделение ряд мера число стимулирование:4 колёсный размер:4 продажа:2 физический лицо производство территория\n"
     ]
    }
   ],
   "source": [
    "print(n_per, n_loc, n_org)\n",
    "print(articles_wv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
