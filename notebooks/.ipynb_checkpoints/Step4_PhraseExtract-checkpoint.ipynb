{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sys.path.append('../src')\n",
    "from encode import *\n",
    "from phrase_extract import PhraseExtracter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Извлечение фраз**\n",
    "\n",
    "Сформируем таблицу разбиения новостных статей на фразы, которая будет иметь следующие поля:\n",
    "\n",
    "* **article_id** - идентификатор новостной статьи\n",
    "* **part** - часть статьи ('title', 'snippet', 'text')\n",
    "* **sent_id** - идентификатор предложения\n",
    "* **begin_boundary** - индекс, с которого начинается фраза в предложении \n",
    "* **end_boundary** - индекс, которым заканчивается фраза в предложении\n",
    "* **phrase_id** - идентификатор фразы\n",
    "* **phrase_lemma_id** - идентификатор лемматизированной фразы\n",
    "* **weight** - sig из TopMine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11375it [00:00, 13606.16it/s]\n",
      "22366it [00:19, 1171.78it/s]\n",
      "104855it [02:14, 780.07it/s] \n"
     ]
    }
   ],
   "source": [
    "articles = pd.read_csv('../data/interim/articles_preproc.csv')\n",
    "\n",
    "articles.title_preproc = articles.title_preproc.apply(conllu_encoder)\n",
    "articles.snippet_preproc = articles.snippet_preproc.apply(conllu_encoder)\n",
    "articles.text_preproc = articles.text_preproc.apply(conllu_encoder)\n",
    "\n",
    "title_phrase_extracter = PhraseExtracter().fit(articles.title_preproc)\n",
    "title_phrase_extracter.dump('../models/title_phrase_extract_model')\n",
    "\n",
    "snippet_phrase_extracter = PhraseExtracter().fit(articles.snippet_preproc)\n",
    "snippet_phrase_extracter.dump('../models/snippet_phrase_extract_model')\n",
    "\n",
    "text_phrase_extracter = PhraseExtracter().fit(articles.text_preproc)\n",
    "text_phrase_extracter.dump('../models/text_phrase_extract_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10817it [02:14, 95.19it/s]"
     ]
    }
   ],
   "source": [
    "articles = pd.read_csv('../data/interim/articles_preproc.csv')\n",
    "\n",
    "articles.title_preproc = articles.title_preproc.apply(conllu_encoder)\n",
    "articles.snippet_preproc = articles.snippet_preproc.apply(conllu_encoder)\n",
    "articles.text_preproc = articles.text_preproc.apply(conllu_encoder)\n",
    "\n",
    "title_phrase_extracter = PhraseExtracter(path='../models/title_phrase_extract_model')\n",
    "snippet_phrase_extracter = PhraseExtracter(path='../models/snippet_phrase_extract_model')\n",
    "text_phrase_extracter = PhraseExtracter(path='../models/text_phrase_extract_model')\n",
    "\n",
    "articles_phrase = []\n",
    "for _, article in tqdm(articles.iterrows()):\n",
    "    \n",
    "    #title\n",
    "    for sent in article.title_preproc:\n",
    "        for begin_boundary, end_boundary, phrase, phrase_lemma, sig in title_phrase_extracter.transform(sent.tokens):\n",
    "            articles_phrase.append((article.id, 'title', sent.id, begin_boundary, end_boundary, phrase, phrase_lemma, sig))\n",
    "            \n",
    "    #snippet\n",
    "    for sent in article.snippet_preproc:\n",
    "        for begin_boundary, end_boundary, phrase, phrase_lemma, sig in snippet_phrase_extracter.transform(sent.tokens):\n",
    "            articles_phrase.append((article.id, 'snippet', sent.id, begin_boundary, end_boundary, phrase, phrase_lemma, sig))\n",
    "            \n",
    "    #text\n",
    "    for sent in article.text_preproc:\n",
    "        for begin_boundary, end_boundary, phrase, phrase_lemma, sig in text_phrase_extracter.transform(sent.tokens):\n",
    "            articles_phrase.append((article.id, 'text', sent.id, begin_boundary, end_boundary, phrase, phrase_lemma, sig))\n",
    "            \n",
    "articles_phrase = pd.DataFrame(articles_phrase, columns=['article_id', 'part', 'sent_id', 'begin_boundary', 'end_boundary', \n",
    "                                                         'phrase_id', 'phrase_lemma_id', 'sig'])\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder().fit(articles_phrase.phrase)\n",
    "articles_phrase.phrase = label_encoder.transform(articles_phrase.phrase)\n",
    "\n",
    "phrases = label_encoder.classes_\n",
    "phrases = pd.DataFrame({'id':range(len(phrases)), 'phrase':phrases})\n",
    "\n",
    "label_encoder = LabelEncoder().fit(articles_phrase.phrase_lemma)\n",
    "articles_phrase.phrase_lemma = label_encoder.transform(articles_phrase.phrase_lemma)\n",
    "\n",
    "phrases_lemma = label_encoder.classes_\n",
    "phrases_lemma = pd.DataFrame({'id':range(len(phrases_lemma)), 'phrase':phrases_lemma})\n",
    "\n",
    "phrases.to_csv('../data/interim/phrases.csv', index=False)\n",
    "phrases_lemma.to_csv('../data/interim/phrases_lemma.csv', index=False)\n",
    "articles_phrase.to_csv('../data/interim/articles_phrase.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/interim/phrases.txt', 'w') as fl:\n",
    "    fl.write('\\n'.join(phrases.phrase))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
